# spark本地环境搭建 #
## 一．	预备条件 ##

安装hadoop,见安装手册 hadoop开发环境搭建.docx。

## 二．spark-2.0.2-bin-hadoop2.7.tgz解压 ##

将当前目录下的spark-2.0.2-bin-hadoop2.7.tgz解压到本地磁盘某个目录下.如D:\tools\hadoop\spark-2.0.2-bin-hadoop2.7。spark-2.0.2针对hadoop2.7。

## 三.环境变量配置 ##

//注意需要配置在用户变量中
path: D:\tools\hadoop\spark-2.0.2-bin-hadoop2.7\bin;D:\tools\hadoop\spark-2.0.2-bin-hadoop2.7\sbin;

## 三. 控制台输入 spark-shell ##

如果Spark安装正确，就能够在控制台的输出中看到如下信息。

    C:\Users\pengqb>spark-shell
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    Setting default log level to "WARN".
    To adjust logging level use sc.setLogLevel(newLevel).
    16/12/05 14:43:53 WARN SparkContext: Use an existing SparkContext, some configur
    ation may not take effect.
    Spark context Web UI available at http://192.168.3.234:4040
    Spark context available as 'sc' (master = local[*], app id = local-1480920232713
    ).
    Spark session available as 'spark'.
    Welcome to
      ____  __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
       /___/ .__/\_,_/_/ /_/\_\   version 2.0.2
      /_/
    
    Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)
    Type in expressions to have them evaluated.
    Type :help for more information.

可以键入如下命令检查Spark Shell是否工作正常。

    sc.version

（或）

    sc.appName

完成上述步骤之后，可以键入如下命令退出Spark Shell窗口：

    :quit

Spark示例应用

完成Spark安装并启动后，就可以用Spark API执行数据分析查询了。首先让我们用Spark API运行流行的Word Count示例。如果还没有运行Spark Scala Shell，首先打开一个Scala Shell窗口。这个示例的相关命令如下所示：

    import org.apache.spark.SparkContext
    import org.apache.spark.SparkContext._
     
    val txtFile = "README.md"
	val txtFile = "D:/tools/hadoop/spark-2.0.2-bin-hadoop2.7/README.md"
    val txtData = sc.textFile(txtFile)
    txtData.cache()

我们可以调用cache函数将上一步生成的RDD对象保存到缓存中，在此之后Spark就不需要在每次数据查询时都重新计算。需要注意的是，cache()是一个延迟操作。在我们调用cache时，Spark并不会马上将数据存储到内存中。只有当在某个RDD上调用一个行动时，才会真正执行这个操作。

现在，我们可以调用count函数，看一下在文本文件中有多少行数据。

    txtData.count()

然后，我们可以执行如下命令进行字数统计。在文本文件中统计数据会显示在每个单词的后面。

    val wcData = txtData.flatMap(l => l.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
    wcData.collect().foreach(println)

如果想查看更多关于如何使用Spark核心API的代码示例，请参考网站上的Spark文档。

# 参考文档 #
用Apache Spark进行大数据处理——第一部分：入门介绍

    http://www.infoq.com/cn/articles/apache-spark-introduction

用Apache Spark进行大数据处理——第二部分：Spark SQL

    http://www.infoq.com/cn/articles/apache-spark-sql

用Apache Spark进行大数据处理——第三部分：Spark流

	http://www.infoq.com/cn/articles/apache-spark-streaming